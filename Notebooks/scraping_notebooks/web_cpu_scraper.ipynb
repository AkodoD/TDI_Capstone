{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import urllib.request\n",
    "import time\n",
    "import string as st\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    return a url if good and log the error otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                print(resp.content)\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "def adaptive_get(url,t):\n",
    "    \"times get requests as to not trip rate limits, returns soup of page or None\"\n",
    "    time.sleep(t)\n",
    "    html_text = simple_get(url)\n",
    "    \n",
    "    if html_text == None:\n",
    "        html_text = try_again(url) #try again returns html text\n",
    "        t += 10\n",
    "    \n",
    "    'if try_again fails to get the link then return none'\n",
    "    if html_text == None:\n",
    "        notify(f\"URL not captured:{url}\")\n",
    "        return None\n",
    "    else:\n",
    "        return (soup(html_text),t)\n",
    "    \n",
    "def try_again(url):\n",
    "    \"waits for a longer period of time so download limit can reset\"\n",
    "    notify(f\"Rate limit hit. Taking break\")\n",
    "    time.sleep(2500)\n",
    "    \n",
    "    html_text = simple_get(url)\n",
    "    if html_text == None:\n",
    "        notify(f\"Rate limit hit AGAIN :( : Pausing for longer\")\n",
    "        time.sleep(5000)\n",
    "        html_text = simple_get(url)\n",
    "    \n",
    "    return html_text\n",
    "\n",
    "def notify(string):\n",
    "    \"prints notifications to a file containing progress and the screen\"\n",
    "    print(string)\n",
    "    with open(\"progress.txt\", 'a') as f:\n",
    "        f.write(string+\"\\n\")\n",
    "        \n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    returns true if response is good and html is found\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    print the error\n",
    "    \"\"\"\n",
    "    notify(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_processors():\n",
    "    \"This is the code to pull the gpu names and the link to each gpu. Since df was saved it doesn't need run again.\"\n",
    "    brands = ['Intel','AMD']\n",
    "    years = list(range(2000,2020))\n",
    "    base = \"https://www.techpowerup.com\"\n",
    "    t=15\n",
    "    cpu_list = []\n",
    "    for brand in brands:\n",
    "        for year in years:\n",
    "            notify(f\"Starting {brand} {year}\")\n",
    "            page_soup,t = adaptive_get(f\"https://www.techpowerup.com/cpudb/?mfgr={brand}&released={year}&sort=name\",t)\n",
    "            relevant_lines = page_soup.select(\"table.processors a\")\n",
    "            if len(relevant_lines) > 0:\n",
    "                for line in relevant_lines:\n",
    "                    cpu_name = line.string\n",
    "                    cpu_link = \"\".join([base,line['href']])\n",
    "                    cpu_list.append({'name': cpu_name,\"brand\":brand,'link':cpu_link})\n",
    "    return cpu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_details(processor_list):\n",
    "    detail_list = []\n",
    "    sleep_time=15\n",
    "    for i,entry in enumerate(processor_list):\n",
    "        new_entry = entry.copy()\n",
    "        if i%5 == 0:\n",
    "            notify(f\"{i/len(processor_list)*100:.1f}% Complete\")\n",
    "            \n",
    "        link = entry['link']\n",
    "\n",
    "        \"get html from link\"\n",
    "        card_soup, sleep_time = adaptive_get(link, sleep_time)\n",
    "        \n",
    "        \"add available information from website to processor database\"\n",
    "        cpu_details = card_soup.select('div.sectioncontainer tr')\n",
    "        for detail in cpu_details:\n",
    "            try:\n",
    "                key = str(detail.select(\"th\")[0].text.strip())\n",
    "                value = str(detail.select(\"td\")[0].text.strip())\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            if key != [] and value != []:\n",
    "                new_entry.update({key:value})\n",
    "                \n",
    "        detail_list.append(new_entry)\n",
    "        \n",
    "        with open('ongoing_collected_data.json', 'a') as f:\n",
    "            json.dump(detail_list[-1], f)\n",
    "        \n",
    "    return detail_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"get all processor links\"\n",
    "cpu_links = find_processors()\n",
    "with open('cpu_urls.json', 'w') as f:\n",
    "    json.dump(cpu_links,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'get links to individual cpus'\n",
    "if 'cpu_list' not in locals():\n",
    "    with open('cpu_urls.json', 'r') as f:\n",
    "        cpu_list = json.load(f)\n",
    "        \n",
    "'fill in details on cpus in list using the output of find_processors'\n",
    "full_details_list = cpu_details(cpu_list)\n",
    "\n",
    "with open('final_cpu_list.json', 'w') as f:\n",
    "    json.dump(full_details_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
